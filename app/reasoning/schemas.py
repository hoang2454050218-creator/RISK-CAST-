"""
Multi-layer reasoning schemas.

These schemas define the data structures for the 6-layer reasoning architecture:
1. FACTUAL    - What is happening?
2. TEMPORAL   - When and timing effects?
3. CAUSAL     - Why is this happening?
4. COUNTERFACTUAL - What if I recommended differently?
5. STRATEGIC  - Fits customer's strategy?
6. META       - Should I decide or escalate?

Each layer produces typed output that builds on previous layers.
"""

from datetime import datetime
from typing import Optional, Any, Tuple
from pydantic import BaseModel, Field, computed_field
from enum import Enum
import uuid


class ReasoningLayer(str, Enum):
    """The 6 reasoning layers in order of execution."""
    FACTUAL = "factual"
    TEMPORAL = "temporal"
    CAUSAL = "causal"
    COUNTERFACTUAL = "counterfactual"
    STRATEGIC = "strategic"
    META = "meta"


# ============================================================================
# BASE LAYER OUTPUT
# ============================================================================


class LayerOutput(BaseModel):
    """
    Base class for output from a single reasoning layer.
    
    All layers produce this structure with layer-specific extensions.
    """
    layer: ReasoningLayer = Field(description="Which layer produced this output")
    started_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="When layer execution started"
    )
    completed_at: Optional[datetime] = Field(
        default=None,
        description="When layer execution completed"
    )
    duration_ms: int = Field(
        default=0,
        ge=0,
        description="Execution time in milliseconds"
    )
    
    # Input to this layer
    inputs: dict = Field(
        default_factory=dict,
        description="Summary of inputs to this layer"
    )
    
    # Output from this layer
    outputs: dict = Field(
        default_factory=dict,
        description="Summary of outputs from this layer"
    )
    
    # Confidence in this layer's output
    confidence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Confidence in this layer's output (0-1)"
    )
    
    # Warnings or flags
    warnings: list[str] = Field(
        default_factory=list,
        description="Warnings generated by this layer"
    )
    
    # Dependencies on other layers
    depends_on: list[ReasoningLayer] = Field(
        default_factory=list,
        description="Layers this output depends on"
    )


# ============================================================================
# LAYER 1: FACTUAL
# ============================================================================


class VerifiedFact(BaseModel):
    """A single verified fact."""
    fact_id: str = Field(default_factory=lambda: f"fact_{uuid.uuid4().hex[:8]}")
    fact_type: str = Field(description="Type of fact (e.g., 'signal_probability', 'vessel_count')")
    value: Any = Field(description="The fact value")
    verified: bool = Field(default=True, description="Whether this fact is verified")
    source: str = Field(description="Source of this fact")
    source_reliability: float = Field(
        ge=0.0, le=1.0,
        description="Reliability of the source (0-1)"
    )
    timestamp: datetime = Field(
        default_factory=datetime.utcnow,
        description="When this fact was observed"
    )


class FactualLayerOutput(LayerOutput):
    """
    Output from Layer 1: Factual Reasoning.
    
    Responsibilities:
    - Gather and validate facts from signal, reality, and context
    - Assess overall data quality
    - Identify data gaps that could affect decision
    - Rate source credibility
    """
    layer: ReasoningLayer = ReasoningLayer.FACTUAL
    
    # Verified facts
    verified_facts: list[VerifiedFact] = Field(
        default_factory=list,
        description="List of verified facts from all sources"
    )
    
    # Data quality assessment
    data_quality_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Overall data quality score (0-1)"
    )
    
    # Data gaps identified
    data_gaps: list[str] = Field(
        default_factory=list,
        description="Missing or stale data that could affect decision"
    )
    
    # Source credibility
    source_credibility: dict[str, float] = Field(
        default_factory=dict,
        description="Credibility score (0-1) for each data source"
    )
    
    @computed_field
    @property
    def fact_count(self) -> int:
        """Number of verified facts."""
        return len(self.verified_facts)
    
    @computed_field
    @property
    def has_critical_gaps(self) -> bool:
        """Whether there are critical data gaps."""
        critical_keywords = ["no real-time", "missing", "outdated", "unavailable"]
        return any(
            any(kw in gap.lower() for kw in critical_keywords)
            for gap in self.data_gaps
        )


# ============================================================================
# LAYER 2: TEMPORAL
# ============================================================================


class TimelineEvent(BaseModel):
    """An event on the timeline."""
    event_id: str = Field(default_factory=lambda: f"evt_{uuid.uuid4().hex[:8]}")
    event_type: str = Field(description="Type of event")
    timestamp: datetime = Field(description="When the event occurs/occurred")
    description: str = Field(description="Event description")
    is_past: bool = Field(description="Whether event has already occurred")
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="Confidence in this event timing"
    )


class TemporalLayerOutput(LayerOutput):
    """
    Output from Layer 2: Temporal Reasoning.
    
    Responsibilities:
    - Construct event timeline
    - Calculate decision deadlines
    - Determine action deadlines
    - Map time-dependent options
    - Assess urgency
    """
    layer: ReasoningLayer = ReasoningLayer.TEMPORAL
    depends_on: list[ReasoningLayer] = [ReasoningLayer.FACTUAL]
    
    # Timeline
    event_sequence: list[TimelineEvent] = Field(
        default_factory=list,
        description="Sequence of events in chronological order"
    )
    
    # Deadlines
    decision_deadline: datetime = Field(
        description="When a decision must be made"
    )
    action_deadlines: dict[str, datetime] = Field(
        default_factory=dict,
        description="Deadline for each possible action (action_type -> deadline)"
    )
    
    # Time-dependent options
    options_by_time: dict[str, list[str]] = Field(
        default_factory=dict,
        description="Available actions at each time horizon"
    )
    
    # Urgency assessment
    urgency_level: str = Field(
        description="Urgency level: immediate, urgent, soon, watch"
    )
    urgency_reason: str = Field(
        description="Why this urgency level was assigned"
    )
    
    # Hours until key milestones
    hours_until_decision_deadline: float = Field(
        default=0.0,
        description="Hours remaining until decision deadline"
    )
    
    @computed_field
    @property
    def is_time_critical(self) -> bool:
        """Whether this decision is time-critical."""
        return self.urgency_level in ["immediate", "urgent"]


# ============================================================================
# LAYER 3: CAUSAL
# ============================================================================


class CausalLink(BaseModel):
    """A single causal link in the chain."""
    link_id: str = Field(default_factory=lambda: f"link_{uuid.uuid4().hex[:8]}")
    cause: str = Field(description="The cause")
    effect: str = Field(description="The effect")
    strength: float = Field(
        ge=0.0, le=1.0,
        description="Strength of causal relationship (0-1)"
    )
    evidence: list[str] = Field(
        default_factory=list,
        description="Evidence supporting this causal link"
    )
    mechanism: Optional[str] = Field(
        default=None,
        description="Mechanism by which cause leads to effect"
    )


class InterventionPoint(BaseModel):
    """A point where intervention is possible."""
    point_id: str = Field(default_factory=lambda: f"int_{uuid.uuid4().hex[:8]}")
    name: str = Field(description="Name of intervention point")
    description: str = Field(description="What intervention would do")
    effectiveness: float = Field(
        ge=0.0, le=1.0,
        description="Expected effectiveness (0-1)"
    )
    cost_estimate_usd: float = Field(
        ge=0,
        description="Estimated cost of intervention"
    )
    feasibility: str = Field(description="Feasibility: high, medium, low")


class CausalLayerOutput(LayerOutput):
    """
    Output from Layer 3: Causal Reasoning.
    
    Responsibilities:
    - Identify root causes
    - Build causal chain
    - Find intervention points
    - Identify confounders
    """
    layer: ReasoningLayer = ReasoningLayer.CAUSAL
    depends_on: list[ReasoningLayer] = [ReasoningLayer.FACTUAL]
    
    # Root causes
    root_causes: list[str] = Field(
        default_factory=list,
        description="Identified root causes of the situation"
    )
    
    # Causal chain
    causal_chain: list[CausalLink] = Field(
        default_factory=list,
        description="Chain of causal links from root cause to impact"
    )
    
    # Intervention points
    intervention_points: list[InterventionPoint] = Field(
        default_factory=list,
        description="Points where customer can intervene"
    )
    
    # Confounders
    confounders: list[str] = Field(
        default_factory=list,
        description="Factors that could confound causal reasoning"
    )
    
    @computed_field
    @property
    def has_actionable_interventions(self) -> bool:
        """Whether there are feasible intervention points."""
        return any(p.feasibility in ["high", "medium"] for p in self.intervention_points)


# ============================================================================
# LAYER 4: COUNTERFACTUAL
# ============================================================================


class Scenario(BaseModel):
    """A possible scenario for counterfactual analysis."""
    scenario_id: str = Field(default_factory=lambda: f"scn_{uuid.uuid4().hex[:8]}")
    name: str = Field(description="Scenario name (e.g., 'base', 'optimistic', 'pessimistic')")
    description: str = Field(description="Scenario description")
    probability: float = Field(
        ge=0.0, le=1.0,
        description="Probability of this scenario"
    )
    assumptions: list[str] = Field(
        default_factory=list,
        description="Key assumptions for this scenario"
    )
    outcomes: dict[str, Any] = Field(
        default_factory=dict,
        description="Expected outcomes under this scenario"
    )
    best_action: str = Field(description="Best action under this scenario")


class CounterfactualLayerOutput(LayerOutput):
    """
    Output from Layer 4: Counterfactual Reasoning.
    
    Responsibilities:
    - Generate alternative scenarios
    - Perform regret analysis
    - Find robust actions
    - Calculate decision boundaries
    """
    layer: ReasoningLayer = ReasoningLayer.COUNTERFACTUAL
    depends_on: list[ReasoningLayer] = [
        ReasoningLayer.FACTUAL,
        ReasoningLayer.TEMPORAL,
        ReasoningLayer.CAUSAL,
    ]
    
    # Alternative scenarios
    scenarios: list[Scenario] = Field(
        default_factory=list,
        description="Alternative scenarios analyzed"
    )
    
    # Regret analysis
    regret_matrix: dict[str, dict[str, float]] = Field(
        default_factory=dict,
        description="Regret for each action under each scenario: action -> scenario -> regret"
    )
    
    # Robust action
    robust_action: Optional[str] = Field(
        default=None,
        description="Action that performs well across all scenarios"
    )
    robustness_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="How robust the recommended action is (0-1)"
    )
    
    # Decision boundaries
    decision_boundaries: dict[str, float] = Field(
        default_factory=dict,
        description="Input values at which decision would flip"
    )
    
    # Minimum regret action
    minimax_regret_action: Optional[str] = Field(
        default=None,
        description="Action that minimizes maximum regret"
    )
    
    @computed_field
    @property
    def is_decision_robust(self) -> bool:
        """Whether the decision is robust to uncertainty."""
        return self.robustness_score >= 0.6


# ============================================================================
# LAYER 5: STRATEGIC
# ============================================================================


class StrategicLayerOutput(LayerOutput):
    """
    Output from Layer 5: Strategic Reasoning.
    
    Responsibilities:
    - Assess alignment with customer's risk tolerance
    - Evaluate long-term impacts
    - Consider relationship effects
    - Check portfolio concentration
    """
    layer: ReasoningLayer = ReasoningLayer.STRATEGIC
    depends_on: list[ReasoningLayer] = [ReasoningLayer.COUNTERFACTUAL]
    
    # Strategy alignment
    risk_tolerance_alignment: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="How well recommended action aligns with customer risk tolerance"
    )
    
    # Impact assessment
    long_term_impact: str = Field(
        default="neutral",
        description="Expected long-term impact: positive, neutral, negative"
    )
    long_term_impact_details: str = Field(
        default="",
        description="Details of long-term impact"
    )
    
    # Relationship effects
    relationship_impact: str = Field(
        default="neutral",
        description="Impact on carrier/supplier relationships"
    )
    relationship_considerations: list[str] = Field(
        default_factory=list,
        description="Specific relationship considerations"
    )
    
    # Portfolio view
    portfolio_exposure: float = Field(
        default=0.0,
        ge=0.0,
        description="Total portfolio exposure as percentage"
    )
    concentration_risk: str = Field(
        default="low",
        description="Concentration risk level: low, medium, high, critical"
    )
    
    # Strategic recommendation
    strategic_override: Optional[str] = Field(
        default=None,
        description="If strategic considerations suggest different action"
    )
    strategic_override_reason: Optional[str] = Field(
        default=None,
        description="Reason for strategic override"
    )
    
    @computed_field
    @property
    def has_strategic_conflict(self) -> bool:
        """Whether there's a conflict between tactical and strategic recommendations."""
        return self.strategic_override is not None


# ============================================================================
# LAYER 6: META
# ============================================================================


class MetaLayerOutput(LayerOutput):
    """
    Output from Layer 6: Meta Reasoning.
    
    The CRITICAL layer that decides whether to decide.
    
    Responsibilities:
    - Assess overall reasoning quality
    - Decide: proceed with decision OR escalate to human
    - Set confidence in the decision process itself
    """
    layer: ReasoningLayer = ReasoningLayer.META
    depends_on: list[ReasoningLayer] = [
        ReasoningLayer.FACTUAL,
        ReasoningLayer.TEMPORAL,
        ReasoningLayer.CAUSAL,
        ReasoningLayer.COUNTERFACTUAL,
        ReasoningLayer.STRATEGIC,
    ]
    
    # Decision to decide
    should_decide: bool = Field(
        description="Whether system should proceed with decision (vs escalate)"
    )
    decision_reason: str = Field(
        description="Why system decided to proceed or escalate"
    )
    
    # If not deciding (escalation)
    escalation_trigger: Optional[str] = Field(
        default=None,
        description="What triggered escalation"
    )
    escalation_reason: Optional[str] = Field(
        default=None,
        description="Detailed reason for escalation"
    )
    escalation_urgency: Optional[str] = Field(
        default=None,
        description="How urgently human review is needed"
    )
    
    # Confidence in reasoning process
    reasoning_confidence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Confidence in the entire reasoning process"
    )
    
    # Quality flags
    reasoning_quality_flags: list[str] = Field(
        default_factory=list,
        description="Quality concerns about the reasoning process"
    )
    
    # Recommended next steps
    if_decide: dict = Field(
        default_factory=dict,
        description="What to do if proceeding with decision"
    )
    if_escalate: dict = Field(
        default_factory=dict,
        description="What to do if escalating"
    )
    
    # Layer quality scores
    layer_quality_scores: dict[str, float] = Field(
        default_factory=dict,
        description="Quality score for each layer"
    )
    
    @computed_field
    @property
    def quality_score(self) -> float:
        """Overall reasoning quality score."""
        if not self.layer_quality_scores:
            return 0.0
        return sum(self.layer_quality_scores.values()) / len(self.layer_quality_scores)


# ============================================================================
# REASONING TRACE
# ============================================================================


class ReasoningTrace(BaseModel):
    """
    Complete reasoning trace through all 6 layers.
    
    This is the FULL record of how a decision was made.
    Every decision has an immutable reasoning trace.
    """
    trace_id: str = Field(
        default_factory=lambda: f"trace_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}"
    )
    started_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = Field(default=None)
    total_duration_ms: int = Field(default=0, ge=0)
    
    # Layer outputs in order
    factual: Optional[FactualLayerOutput] = Field(default=None)
    temporal: Optional[TemporalLayerOutput] = Field(default=None)
    causal: Optional[CausalLayerOutput] = Field(default=None)
    counterfactual: Optional[CounterfactualLayerOutput] = Field(default=None)
    strategic: Optional[StrategicLayerOutput] = Field(default=None)
    meta: Optional[MetaLayerOutput] = Field(default=None)
    
    # Final outputs
    final_decision: Optional[str] = Field(
        default=None,
        description="Final decision (None if escalated)"
    )
    final_confidence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Confidence in final decision"
    )
    escalated: bool = Field(
        default=False,
        description="Whether decision was escalated to human"
    )
    escalation_reason: Optional[str] = Field(
        default=None,
        description="Reason for escalation"
    )
    
    # Quality metrics
    data_quality_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Data quality from factual layer"
    )
    reasoning_quality_score: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Overall reasoning quality"
    )
    
    # LLM Enhancement fields
    llm_enhanced: bool = Field(
        default=False,
        description="Whether LLM was used to enhance reasoning"
    )
    llm_explanation: Optional[str] = Field(
        default=None,
        description="LLM-generated natural language explanation of the decision"
    )
    llm_causal_enhancement: Optional[dict] = Field(
        default=None,
        description="LLM-enhanced causal analysis (additional context, historical precedents)"
    )
    
    def get_layer(self, layer: ReasoningLayer) -> Optional[LayerOutput]:
        """Get output for a specific layer."""
        layer_map = {
            ReasoningLayer.FACTUAL: self.factual,
            ReasoningLayer.TEMPORAL: self.temporal,
            ReasoningLayer.CAUSAL: self.causal,
            ReasoningLayer.COUNTERFACTUAL: self.counterfactual,
            ReasoningLayer.STRATEGIC: self.strategic,
            ReasoningLayer.META: self.meta,
        }
        return layer_map.get(layer)
    
    def get_all_warnings(self) -> list[str]:
        """Get all warnings from all layers."""
        warnings = []
        for layer_output in [
            self.factual,
            self.temporal,
            self.causal,
            self.counterfactual,
            self.strategic,
            self.meta,
        ]:
            if layer_output:
                warnings.extend(layer_output.warnings)
        return warnings
    
    def get_layer_summary(self) -> dict[str, dict]:
        """Get summary of all layers."""
        summary = {}
        for layer in ReasoningLayer:
            output = self.get_layer(layer)
            if output:
                summary[layer.value] = {
                    "confidence": output.confidence,
                    "duration_ms": output.duration_ms,
                    "warning_count": len(output.warnings),
                }
        return summary
    
    @computed_field
    @property
    def layer_count(self) -> int:
        """Number of layers executed."""
        return sum(1 for layer in ReasoningLayer if self.get_layer(layer) is not None)
    
    @computed_field
    @property
    def is_complete(self) -> bool:
        """Whether all layers have been executed."""
        return self.layer_count == len(ReasoningLayer)
