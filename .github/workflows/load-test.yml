# RISKCAST Load Testing Pipeline
# B4 COMPLIANCE: "Load test integration in CI not automated" - FIXED
#
# This workflow runs automated load tests using Locust with:
# - Real HTTP client requests (D2 compliance)
# - SLA compliance checking
# - Multiple test scenarios
# - Artifact collection for results

name: Load Tests

on:
  # Run on main branch pushes (after deploy)
  push:
    branches: [main]
  
  # Run on PRs to main
  pull_request:
    branches: [main]
  
  # Scheduled nightly runs
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily
  
  # Manual trigger for on-demand testing
  workflow_dispatch:
    inputs:
      users:
        description: 'Number of concurrent users'
        required: false
        default: '50'
      duration:
        description: 'Test duration (e.g., 60s, 5m)'
        required: false
        default: '60s'
      spawn_rate:
        description: 'Users to spawn per second'
        required: false
        default: '5'
      test_type:
        description: 'Test type'
        required: false
        default: 'smoke'
        type: choice
        options:
          - smoke
          - load
          - stress
          - spike

env:
  PYTHON_VERSION: "3.11"
  # SLA Thresholds
  SLA_P95_LATENCY_MS: 500
  SLA_P99_LATENCY_MS: 1000
  SLA_SUCCESS_RATE: 99

jobs:
  # ===========================================================================
  # Smoke Test (Quick validation)
  # ===========================================================================
  smoke-test:
    name: Smoke Test
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event.inputs.test_type == 'smoke'
    
    services:
      # Start the application in Docker
      app:
        image: ghcr.io/${{ github.repository }}:${{ github.sha }}
        credentials:
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
        ports:
          - 8000:8000
        env:
          ENVIRONMENT: testing
          DATABASE_URL: postgresql+asyncpg://riskcast:riskcast@postgres:5432/riskcast_test
          REDIS_URL: redis://redis:6379/0
        options: >-
          --health-cmd "curl -f http://localhost:8000/api/v1/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: riskcast
          POSTGRES_PASSWORD: riskcast
          POSTGRES_DB: riskcast_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Locust
        run: |
          python -m pip install --upgrade pip
          pip install locust

      - name: Wait for services
        run: |
          echo "Waiting for services to be ready..."
          sleep 10
          curl -f http://localhost:8000/api/v1/health || exit 1

      - name: Run Smoke Test
        env:
          LOCUST_FAIL_ON_SLA: "true"
          LOCUST_SLA_RESULTS: "smoke_results.json"
        run: |
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users 5 \
            --spawn-rate 1 \
            --run-time 30s \
            --csv=smoke \
            --html=smoke_report.html \
            --only-summary \
            HealthCheckUser DecisionUser

      - name: Check SLA Results
        id: sla_check
        run: |
          if [ -f smoke_results.json ]; then
            SLA_PASSED=$(python -c "import json; print(json.load(open('smoke_results.json'))['passed'])")
            echo "sla_passed=$SLA_PASSED" >> $GITHUB_OUTPUT
            if [ "$SLA_PASSED" != "True" ]; then
              echo "::error::SLA check failed"
              exit 1
            fi
          fi

      - name: Upload Smoke Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-results
          path: |
            smoke*.csv
            smoke_report.html
            smoke_results.json

  # ===========================================================================
  # Load Test (Normal traffic simulation)
  # ===========================================================================
  load-test:
    name: Load Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.test_type == 'load'
    needs: [smoke-test]
    # Continue even if smoke test is skipped
    continue-on-error: false

    services:
      app:
        image: ghcr.io/${{ github.repository }}:${{ github.sha }}
        credentials:
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
        ports:
          - 8000:8000
        env:
          ENVIRONMENT: testing
          DATABASE_URL: postgresql+asyncpg://riskcast:riskcast@postgres:5432/riskcast_test
          REDIS_URL: redis://redis:6379/0

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: riskcast
          POSTGRES_PASSWORD: riskcast
          POSTGRES_DB: riskcast_test

      redis:
        image: redis:7-alpine

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install locust matplotlib pandas

      - name: Wait for services
        run: |
          echo "Waiting for services..."
          sleep 15
          for i in {1..30}; do
            if curl -sf http://localhost:8000/api/v1/health; then
              echo "Service is ready"
              break
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done

      - name: Run Load Test
        env:
          LOCUST_FAIL_ON_SLA: "true"
          LOCUST_SLA_RESULTS: "load_results.json"
        run: |
          USERS=${{ github.event.inputs.users || '50' }}
          DURATION=${{ github.event.inputs.duration || '60s' }}
          SPAWN_RATE=${{ github.event.inputs.spawn_rate || '5' }}
          
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users $USERS \
            --spawn-rate $SPAWN_RATE \
            --run-time $DURATION \
            --csv=load \
            --html=load_report.html \
            MixedWorkloadUser

      - name: Generate Performance Charts
        if: always()
        run: |
          python << 'EOF'
          import pandas as pd
          import matplotlib.pyplot as plt
          import json
          import os
          
          # Load stats
          try:
              stats_df = pd.read_csv('load_stats.csv')
              history_df = pd.read_csv('load_stats_history.csv')
              
              # Create charts
              fig, axes = plt.subplots(2, 2, figsize=(14, 10))
              
              # Response time over time
              if 'Timestamp' in history_df.columns:
                  history_df['Timestamp'] = pd.to_datetime(history_df['Timestamp'], unit='s')
                  axes[0, 0].plot(history_df['Timestamp'], history_df['Total Average Response Time'])
                  axes[0, 0].set_title('Average Response Time Over Time')
                  axes[0, 0].set_ylabel('Response Time (ms)')
                  axes[0, 0].tick_params(axis='x', rotation=45)
              
              # Requests per second
              if 'Total Request Count' in history_df.columns:
                  axes[0, 1].plot(history_df['Timestamp'], history_df['Requests/s'])
                  axes[0, 1].set_title('Requests Per Second')
                  axes[0, 1].set_ylabel('RPS')
                  axes[0, 1].tick_params(axis='x', rotation=45)
              
              # Response time distribution
              if '50%' in stats_df.columns:
                  percentiles = ['50%', '75%', '95%', '99%']
                  available = [p for p in percentiles if p in stats_df.columns]
                  if available:
                      stats_df[available].iloc[0:10].plot(kind='bar', ax=axes[1, 0])
                      axes[1, 0].set_title('Response Time Percentiles by Endpoint')
                      axes[1, 0].set_ylabel('Response Time (ms)')
                      axes[1, 0].tick_params(axis='x', rotation=45)
              
              # Error rate
              if 'Failure Count' in stats_df.columns:
                  error_rates = stats_df['Failure Count'] / (stats_df['Request Count'] + 0.001) * 100
                  axes[1, 1].bar(range(len(error_rates[:10])), error_rates[:10])
                  axes[1, 1].set_title('Error Rate by Endpoint')
                  axes[1, 1].set_ylabel('Error Rate (%)')
                  axes[1, 1].set_xticks(range(len(error_rates[:10])))
                  axes[1, 1].set_xticklabels(stats_df['Name'][:10], rotation=45, ha='right')
              
              plt.tight_layout()
              plt.savefig('load_performance_charts.png', dpi=150)
              print("Charts generated successfully")
          except Exception as e:
              print(f"Chart generation failed: {e}")
          EOF

      - name: Check SLA Compliance
        id: sla
        run: |
          if [ -f load_results.json ]; then
            python << 'EOF'
          import json
          import sys
          
          with open('load_results.json') as f:
              results = json.load(f)
          
          print("=" * 60)
          print("SLA COMPLIANCE REPORT")
          print("=" * 60)
          
          for check, data in results.get('checks', {}).items():
              status = "✅ PASS" if data['passed'] else "❌ FAIL"
              print(f"{check}: {data['value']:.4f} (threshold: {data['threshold']}) {status}")
          
          print("-" * 60)
          print(f"Total Requests: {results.get('total_requests', 0)}")
          print(f"Failed Requests: {results.get('failed_requests', 0)}")
          print(f"Overall: {'✅ PASSED' if results['passed'] else '❌ FAILED'}")
          print("=" * 60)
          
          if not results['passed']:
              print("::error::Load test SLA check failed")
              sys.exit(1)
          EOF
          fi

      - name: Upload Load Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load*.csv
            load_report.html
            load_results.json
            load_performance_charts.png

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let slaResults = {};
            try {
              slaResults = JSON.parse(fs.readFileSync('load_results.json', 'utf8'));
            } catch (e) {
              console.log('No SLA results file found');
              return;
            }
            
            const passed = slaResults.passed ? '✅' : '❌';
            
            let body = `## Load Test Results ${passed}\n\n`;
            body += `| Metric | Value | Threshold | Status |\n`;
            body += `|--------|-------|-----------|--------|\n`;
            
            for (const [name, data] of Object.entries(slaResults.checks || {})) {
              const status = data.passed ? '✅' : '❌';
              body += `| ${name} | ${data.value.toFixed(4)} | ${data.threshold} | ${status} |\n`;
            }
            
            body += `\n**Total Requests:** ${slaResults.total_requests || 0}`;
            body += `\n**Failed Requests:** ${slaResults.failed_requests || 0}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  # ===========================================================================
  # Stress Test (Beyond normal capacity)
  # ===========================================================================
  stress-test:
    name: Stress Test
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'stress' || github.event_name == 'schedule'

    services:
      app:
        image: ghcr.io/${{ github.repository }}:${{ github.sha }}
        credentials:
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
        ports:
          - 8000:8000
        env:
          ENVIRONMENT: testing

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: riskcast
          POSTGRES_PASSWORD: riskcast
          POSTGRES_DB: riskcast_test

      redis:
        image: redis:7-alpine

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install locust

      - name: Wait for services
        run: sleep 20

      - name: Run Stress Test
        env:
          LOCUST_SLA_RESULTS: "stress_results.json"
        run: |
          # Stress test: 2x normal load
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users 100 \
            --spawn-rate 10 \
            --run-time 120s \
            --csv=stress \
            --html=stress_report.html \
            MixedWorkloadUser DecisionUser

      - name: Analyze Stress Results
        run: |
          python << 'EOF'
          import json
          
          try:
              with open('stress_results.json') as f:
                  results = json.load(f)
              
              # Under stress, we allow degradation but track it
              print("Stress Test Analysis")
              print("=" * 40)
              print(f"Success Rate: {results.get('checks', {}).get('success_rate', {}).get('value', 0):.2%}")
              print(f"P99 Latency: {results.get('checks', {}).get('p99_latency_ms', {}).get('value', 0):.0f}ms")
              
              # Stress tests don't fail on SLA - they measure limits
              print("\nNote: Stress test completed. Review results for capacity planning.")
          except FileNotFoundError:
              print("No stress results found")
          EOF

      - name: Upload Stress Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stress-test-results
          path: |
            stress*.csv
            stress_report.html
            stress_results.json

  # ===========================================================================
  # Spike Test (Sudden traffic surge)
  # ===========================================================================
  spike-test:
    name: Spike Test
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'spike'

    services:
      app:
        image: ghcr.io/${{ github.repository }}:${{ github.sha }}
        credentials:
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
        ports:
          - 8000:8000
        env:
          ENVIRONMENT: testing

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: riskcast
          POSTGRES_PASSWORD: riskcast
          POSTGRES_DB: riskcast_test

      redis:
        image: redis:7-alpine

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: pip install locust

      - name: Wait for services
        run: sleep 20

      - name: Run Spike Test
        run: |
          # Phase 1: Baseline (30s at 10 users)
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users 10 \
            --spawn-rate 2 \
            --run-time 30s \
            --csv=spike_baseline \
            MixedWorkloadUser
          
          # Phase 2: Spike (30s at 100 users - 10x)
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users 100 \
            --spawn-rate 50 \
            --run-time 30s \
            --csv=spike_surge \
            MixedWorkloadUser
          
          # Phase 3: Recovery (30s at 10 users)
          locust -f tests/load/locustfile.py \
            --host=http://localhost:8000 \
            --headless \
            --users 10 \
            --spawn-rate 2 \
            --run-time 30s \
            --csv=spike_recovery \
            MixedWorkloadUser

      - name: Analyze Spike Recovery
        run: |
          python << 'EOF'
          import pandas as pd
          
          try:
              baseline = pd.read_csv('spike_baseline_stats.csv')
              surge = pd.read_csv('spike_surge_stats.csv')
              recovery = pd.read_csv('spike_recovery_stats.csv')
              
              print("Spike Test Analysis")
              print("=" * 50)
              
              for name, df in [("Baseline", baseline), ("Spike", surge), ("Recovery", recovery)]:
                  if 'Average Response Time' in df.columns:
                      avg_rt = df['Average Response Time'].mean()
                      print(f"{name} Avg Response Time: {avg_rt:.2f}ms")
              
              # Check recovery - should be similar to baseline
              if 'Average Response Time' in baseline.columns and 'Average Response Time' in recovery.columns:
                  baseline_rt = baseline['Average Response Time'].mean()
                  recovery_rt = recovery['Average Response Time'].mean()
                  
                  if recovery_rt <= baseline_rt * 1.5:
                      print("\n✅ System recovered well from spike")
                  else:
                      print("\n⚠️ Recovery slower than expected")
          except Exception as e:
              print(f"Analysis error: {e}")
          EOF

      - name: Upload Spike Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: spike-test-results
          path: |
            spike*.csv

  # ===========================================================================
  # Summary Job
  # ===========================================================================
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [smoke-test, load-test]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results
        continue-on-error: true

      - name: Generate Summary
        run: |
          echo "# Load Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Check each test result
          for test in smoke load stress spike; do
            if [ -f "results/${test}-test-results/${test}_results.json" ]; then
              PASSED=$(python3 -c "import json; print('✅' if json.load(open('results/${test}-test-results/${test}_results.json'))['passed'] else '❌')" 2>/dev/null || echo "❓")
              echo "- **${test^} Test:** $PASSED" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## SLA Thresholds" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Threshold |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------|" >> $GITHUB_STEP_SUMMARY
          echo "| P95 Latency | ≤ ${{ env.SLA_P95_LATENCY_MS }}ms |" >> $GITHUB_STEP_SUMMARY
          echo "| P99 Latency | ≤ ${{ env.SLA_P99_LATENCY_MS }}ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Success Rate | ≥ ${{ env.SLA_SUCCESS_RATE }}% |" >> $GITHUB_STEP_SUMMARY
